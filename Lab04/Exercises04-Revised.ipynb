{
 "metadata": {
  "name": "",
  "signature": "sha256:bafa4db5514eb61e784f8618d89329a4eb5a266fd0e93dcd9288591c7c4d7738"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "%precision 4\n",
      "plt.style.use('ggplot')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.linalg as la\n",
      "import scipy.stats as st"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Latent Semantic Analysis (LSA) is a method for reducing the dimnesionality of documents treated as a bag of words. It is used for document classification, clustering and retrieval. For example, LSA can be used to search for prior art given a new patent application. In this homework, we will implement a small library for simple latent semantic analysis as a practical example of the application of SVD. The ideas are very similar to PCA.\n",
      "\n",
      "We will implement a toy example of LSA to get familiar with the ideas. If you want to use LSA or similar methods for statiscal language analyis, the most efficient Python library is probably [gensim](https://radimrehurek.com/gensim/) - this also provides an online algorithm - i.e. the training information can be continuously updated. Other useful functions for processing natural language can be found in the [Natural Lnaguage Toolkit](http://www.nltk.org/)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note**: The SVD from scipy.linalg performs a full decomposition, which is inefficient since we only need to decompose until we get the first k singluar values. If the SVD from `scipy.linalg` is too slow, please use the `sparsesvd` function from the [sparsesvd](https://pypi.python.org/pypi/sparsesvd/) package to perform SVD instead.  You can install in the usual way with \n",
      "```\n",
      "!pip install sparsesvd\n",
      "```\n",
      "\n",
      "Then import the following\n",
      "```python\n",
      "from sparsesvd import sparsesvd \n",
      "from scipy.sparse import csc_matrix \n",
      "```\n",
      "\n",
      "and use as follows\n",
      "```python\n",
      "sparsesvd(csc_matrix(M), k=10)\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!pip install sparsesvd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Requirement already satisfied (use --upgrade to upgrade): sparsesvd in /home/bitnami/anaconda/lib/python2.7/site-packages\r\n",
        "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.6.0 in /home/bitnami/anaconda/lib/python2.7/site-packages (from sparsesvd)\r\n",
        "Requirement already satisfied (use --upgrade to upgrade): cython in /home/bitnami/anaconda/lib/python2.7/site-packages (from sparsesvd)\r\n",
        "Cleaning up...\r\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 1 (10 points)**.  Calculating pairwise distance matrices.\n",
      "\n",
      "Suppose we want to construct a distance matrix between the rows of a matrix. For example, given the matrix \n",
      "\n",
      "```python\n",
      "M = np.array([[1,2,3],[4,5,6]])\n",
      "```\n",
      "\n",
      "the distance matrix using Euclidean distance as the measure would be\n",
      "```python\n",
      "[[ 0.000  1.414  2.828]\n",
      " [ 1.414  0.000  1.414]\n",
      " [ 2.828  1.414  0.000]] \n",
      "```\n",
      "if $M$ was a collection of column vectors.\n",
      "\n",
      "Write a function to calculate the pairwise-distance matrix given the matrix $M$ and some arbitrary distance function. Your functions should have the following signature:\n",
      "```\n",
      "def func_name(M, distance_func):\n",
      "    pass\n",
      "```\n",
      "\n",
      "0. Write a distance function for the Euclidean, squared Euclidean and cosine measures.\n",
      "1. Write the function using looping for M as a collection of row vectors.\n",
      "2. Write the function using looping for M as a collection of column vectors.\n",
      "3. Wrtie the function using broadcasting for M as a colleciton of row vectors.\n",
      "4. Write the function using broadcasting for M as a colleciton of column vectors. \n",
      "\n",
      "For 3 and 4, try to avoid using transposition (but if you get stuck, there will be no penalty for using transpoition). Check that all four functions give the same result when applied to the given matrix $M$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here\n",
      "def euclidean(a,b):\n",
      "    return np.linalg.norm(a-b) \n",
      "    \n",
      "def suqare_euclidean(a,b):  \n",
      "    return np.linalg.norm(a-b)**2\n",
      "\n",
      "def cosine(a,b):\n",
      "    return 1- np.dot(a,b.T)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
      "\n",
      "def distance_looping_row(M, distance_func):\n",
      "    distance = np.zeros((M.shape[0],M.shape[0]))\n",
      "    #M.shape[0] refers to the number of rows in M\n",
      "    for i in range(M.shape[0]):\n",
      "        for j in range(i, M.shape[0]):\n",
      "            distance[i, j] = distance_func(M[i,:],M[j,:])\n",
      "    distance = distance + distance.T - np.diag(distance.diagonal())\n",
      "    return distance\n",
      " \n",
      "def distance_looping_column(M, distance_func):\n",
      "    distance = np.zeros((M.shape[1],M.shape[1]))\n",
      "    #M.shape[1] refers to the number of columns in M\n",
      "    for i in range(M.shape[1]):\n",
      "        for j in range(i,M.shape[1]):\n",
      "            distance[i,j] = distance_func(M[:,i],M[:,j])\n",
      "    distance = distance + distance.T - np.diag(distance.diagonal())\n",
      "    return distance\n",
      "\n",
      "A = np.array([[1,2,3],[4,5,6]])\n",
      "print distance_looping_row(A, euclidean)\n",
      "print distance_looping_row(A, suqare_euclidean)\n",
      "print distance_looping_row(A, cosine)\n",
      "print distance_looping_column(A, euclidean)\n",
      "print distance_looping_column(A, suqare_euclidean)\n",
      "print distance_looping_column(A, cosine)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.000  5.196]\n",
        " [ 5.196  0.000]]\n",
        "[[ 0.000  27.000]\n",
        " [ 27.000  0.000]]\n",
        "[[ 0.000  0.025]\n",
        " [ 0.025  0.000]]\n",
        "[[ 0.000  1.414  2.828]\n",
        " [ 1.414  0.000  1.414]\n",
        " [ 2.828  1.414  0.000]]\n",
        "[[ 0.000  2.000  8.000]\n",
        " [ 2.000  0.000  2.000]\n",
        " [ 8.000  2.000  0.000]]\n",
        "[[ 0.000  0.009  0.024]\n",
        " [ 0.009 -0.000  0.003]\n",
        " [ 0.024  0.003  0.000]]\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def distance_broadcasting_row(M, distance):\n",
      "    if distance == euclidean:\n",
      "        return np.sum((M[:,None] - M[None, :])**2, -1)**0.5\n",
      "    elif distance == suqare_euclidean:\n",
      "        return np.sum((M[:,None] - M[None, :])**2, -1)\n",
      "    elif distance == cosine:\n",
      "        N = np.zeros(M.shape)\n",
      "        a = np.sum(M[:,None] * M[None, :], -1)\n",
      "        b = np.sum((M[:,None] - N[None,:])**2, -1)**0.5\n",
      "        c = np.sum((M[None,:] - N[:,None])**2, -1)**0.5\n",
      "        return 1 - a / (b * c)\n",
      "\n",
      "print distance_broadcasting_row(A, euclidean)\n",
      "print distance_broadcasting_row(A, suqare_euclidean)\n",
      "print distance_broadcasting_row(A, cosine)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.000  5.196]\n",
        " [ 5.196  0.000]]\n",
        "[[ 0 27]\n",
        " [27  0]]\n",
        "[[ 0.000  0.025]\n",
        " [ 0.025  0.000]]\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def distance_broadcasting_column(M, distance):\n",
      "    M = M.T\n",
      "    if distance == euclidean:\n",
      "        return np.sum((M[:,None] - M[None, :])**2, -1)**0.5\n",
      "    elif distance == suqare_euclidean:\n",
      "        return np.sum((M[:,None] - M[None, :])**2, -1)\n",
      "    elif distance == cosine:\n",
      "        N = np.zeros(M.shape)\n",
      "        a = np.sum(M[:,None] * M[None, :], -1)\n",
      "        b = np.sum((M[:,None] - N[None,:])**2, -1)**0.5\n",
      "        c = np.sum((M[None,:] - N[:,None])**2, -1)**0.5\n",
      "        return 1 - a / (b * c)\n",
      "\n",
      "print distance_broadcasting_column(A, euclidean)\n",
      "print distance_broadcasting_column(A, suqare_euclidean)\n",
      "print distance_broadcasting_column(A, cosine)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.000  1.414  2.828]\n",
        " [ 1.414  0.000  1.414]\n",
        " [ 2.828  1.414  0.000]]\n",
        "[[0 2 8]\n",
        " [2 0 2]\n",
        " [8 2 0]]\n",
        "[[ 0.000  0.009  0.024]\n",
        " [ 0.009 -0.000  0.003]\n",
        " [ 0.024  0.003  0.000]]\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 2 (10 points)**. Write 3 functions to calculate the term frequency (tf), the inverse document frequency (idf) and the product (tf-idf). Each function should take a single argument `docs`, which is a dictionary of (key=identifier, value=dcoument text) pairs, and return an appropriately sized array. Convert '-' to ' ' (space), remove punctuation, convert text to lowercase and split on whitespace to generate a collection of terms from the dcoument text.\n",
      "\n",
      "- tf = the number of occurrences of term $i$ in document $j$\n",
      "- idf = $\\log \\frac{n}{1 + \\text{df}_i}$ where $n$ is the total number of documents and $\\text{df}_i$ is the number of documents in which term $i$ occurs.\n",
      "\n",
      "Print the table of tf-idf values for the following document collection\n",
      "\n",
      "```\n",
      "s1 = \"The quick brown fox\"\n",
      "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
      "s3 = \"The the the lazy dog elephant.\"\n",
      "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
      "\n",
      "docs = {'s1': s1, 's2': s2, 's3': s3, 's4': s4}\n",
      "```\n",
      "\n",
      "Note: You can use either a numpy array or pandas dataframe to store the matrix. However, we suggest using a Pnadas dataframe since that will allow you to keep track of the row (term) and column (document) names in a single object. Of course, you could also maintain a numpy matrix, a list of terms, and a list of documents separately if you prefer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "s1 = \"The quick brown fox\"\n",
      "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
      "s3 = \"The the the lazy dog elephant.\"\n",
      "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
      "docs = {'s1': s1, 's2': s2, 's3': s3, 's4': s4}\n",
      "\n",
      "from collections import Counter\n",
      "def tf(doc):\n",
      "    \"\"\"Returns the number of times each term occurs in a dcoument.\n",
      "    We preprocess the document to strip punctuation and convert to lowercase.\n",
      "    Terms are found by splitting on whitespace.\"\"\"\n",
      "    # remove punctuation, convert text to lowercase, and split by whitespace to find each term\n",
      "    text = string.replace(doc, '-', ' ')\n",
      "    text = text.lower().replace('\\n', ' ').translate(None, string.punctuation).split(\" \")\n",
      "    # Use Counter in package \"collections\"\n",
      "    count = dict(Counter(text))\n",
      "    return(count)\n",
      "\n",
      "print \"Term frequency for s1:\", tf(s1)\n",
      "print \"Term frequency for s2:\", tf(s2)\n",
      "print \"Term frequency for s3:\", tf(s3)\n",
      "print \"Term frequency for s4:\", tf(s4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Term frequency for s1: {'quick': 1, 'brown': 1, 'the': 1, 'fox': 1}\n",
        "Term frequency for s2: {'brown': 1, 'jumps': 4, 'fox': 1, 'over': 1, 'the': 1}\n",
        "Term frequency for s3: {'the': 3, 'dog': 1, 'lazy': 1, 'elephant': 1}\n",
        "Term frequency for s4: {'tiger': 1, 'peacock': 1, 'lion': 1, 'elephant': 1, 'the': 5, 'dog': 1}\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tfs(docs):\n",
      "    \"\"\"Create a term freqeuncy dataframe from a dictionary of documents.\"\"\"\n",
      "    # row (term) and column (document) \n",
      "    docname = []\n",
      "    termfreq = []\n",
      "    for k, v in docs.iteritems():\n",
      "        docname.append(k)\n",
      "        termfreq.append(tf(v)) \n",
      "    # Create DataFrame from list of dictionaries, with index being corresponding document name\n",
      "    # Then use \".T\" to transpose the dataframe so that the indexes now being the term's name\n",
      "    # Then use \".sort(axis = 1)\" to sort the dataframe by column name so that the results are more readable\n",
      "    # Then use \".fillna(0)\" to substitute all the NaN with 0\n",
      "    return pd.DataFrame(termfreq, index = docname).T.fillna(0)   \n",
      "\n",
      "print tfs(docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "          s3  s2  s1  s4\n",
        "brown      0   1   1   0\n",
        "dog        1   0   0   1\n",
        "elephant   1   0   0   1\n",
        "fox        0   1   1   0\n",
        "jumps      0   4   0   0\n",
        "lazy       1   0   0   0\n",
        "lion       0   0   0   1\n",
        "over       0   1   0   0\n",
        "peacock    0   0   0   1\n",
        "quick      0   0   1   0\n",
        "the        3   1   1   5\n",
        "tiger      0   0   0   1\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def idf(docs):\n",
      "    \"\"\"Find inverse document frequecny series from a dictionry of doucmnets.\"\"\"\n",
      "    # This can be based on the output of function tfs\n",
      "    data = tfs(docs)\n",
      "    # n is the total number of documents\n",
      "    n = float(len(docs))\n",
      "    term = []\n",
      "    df = []\n",
      "    idf = []\n",
      "    for i,v in enumerate(data.index):\n",
      "        term.append(v)\n",
      "        # dfi is the number of documents in which term i occurs\n",
      "        # So just need to locate each row and see how many non-zero values there are\n",
      "        df.append(float(np.count_nonzero(data.iloc[i])))\n",
      "        # Then calculate the value of idf\n",
      "        idf.append(np.log(n/(1+df[-1])))\n",
      "    # Return a readable dataframe\n",
      "    return pd.DataFrame(idf, index = term, columns=['idf'])\n",
      "\n",
      "print idf(docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "               idf\n",
        "brown     0.287682\n",
        "dog       0.287682\n",
        "elephant  0.287682\n",
        "fox       0.287682\n",
        "jumps     0.693147\n",
        "lazy      0.693147\n",
        "lion      0.693147\n",
        "over      0.693147\n",
        "peacock   0.693147\n",
        "quick     0.693147\n",
        "the      -0.223144\n",
        "tiger     0.693147\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tf_idf(docs):\n",
      "    \"\"\"Return the product of the term-frequency and inverse document freqeucny.\"\"\"\n",
      "    termfreq = tfs(docs)\n",
      "    invdocfreq = idf(docs)\n",
      "    for i in range(len(termfreq)):\n",
      "        termfreq.ix[i] = termfreq.ix[i] * invdocfreq.ix[i,0]\n",
      "    return termfreq\n",
      "\n",
      "\n",
      "print tf_idf(docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                s3        s2        s1        s4\n",
        "brown     0.000000  0.287682  0.287682  0.000000\n",
        "dog       0.287682  0.000000  0.000000  0.287682\n",
        "elephant  0.287682  0.000000  0.000000  0.287682\n",
        "fox       0.000000  0.287682  0.287682  0.000000\n",
        "jumps     0.000000  2.772589  0.000000  0.000000\n",
        "lazy      0.693147  0.000000  0.000000  0.000000\n",
        "lion      0.000000  0.000000  0.000000  0.693147\n",
        "over      0.000000  0.693147  0.000000  0.000000\n",
        "peacock   0.000000  0.000000  0.000000  0.693147\n",
        "quick     0.000000  0.000000  0.693147  0.000000\n",
        "the      -0.669431 -0.223144 -0.223144 -1.115718\n",
        "tiger     0.000000  0.000000  0.000000  0.693147\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 3 (10 points)**. \n",
      "\n",
      "1. Write a function that takes a matrix $M$ and an integer $k$ as arguments, and reconstructs a reduced matrix using only the $k$ largest singular values. Use the `scipy.linagl.svd` function to perform the decomposition. This is the least squares approximation to the matrix $M$ in $k$ dimensions.\n",
      "\n",
      "2. Apply the function you just wrote to the following term-frequency matrix for a set of $9$ documents using $k=2$ and print the reconstructed matrix $M'$.\n",
      "```\n",
      "M = np.array([[1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 1, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 1, 1, 2, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 1]])\n",
      "```\n",
      "\n",
      "3. Calculate the pairwise correlation matrix for the original matrix M and the reconstructed matrix using $k=2$ singular values (you may use [scipy.stats.spearmanr](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) to do the calculations). Consider the fist 5 sets of documents as one group $G1$ and the last 4 as another group $G2$ (i.e. first 5 and last 4 columns). What is the average within group correlation for $G1$, $G2$ and the average cross-group correlation for G1-G2 using either $M$ or $M'$. (Do not include self-correlation in the within-group calculations.)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here\n",
      "import scipy\n",
      "from sparsesvd import sparsesvd \n",
      "from scipy.sparse import csc_matrix\n",
      "\n",
      "# 1) Regular svd version (scipy.linagl.svd)\n",
      "def re_regular(M, k):\n",
      "    S = np.zeros((M.shape[0], M.shape[1]))\n",
      "    U, s, V = scipy.linalg.svd(M)\n",
      "    S[:len(s), :len(s)] = np.diag(s)\n",
      "    return np.dot(U[:,0:k], np.dot(S[0:k, 0:k], V[0:k,:]))\n",
      "\n",
      "# 2) Sparse svd version\n",
      "def re_sparse(M, k):\n",
      "    ut, s, vt = sparsesvd(csc_matrix(M), k)\n",
      "    return np.dot(ut.T, np.dot(np.diag(s), vt))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = np.array([[1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "    [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "    [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 1, 1, 0, 1, 0, 0, 0, 0],\n",
      "    [0, 1, 1, 2, 0, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "    [0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
      "    [0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 1, 1]])\n",
      "\n",
      "reMregular = re_regular(M, 2)\n",
      "reMsparse = re_sparse(M, 2)\n",
      "\n",
      "print \"Reconstructed matrix M' by scipy.linagl.svd is:\", reMregular\n",
      "print \"Reconstructed matrix M' by sparsesvd is:\", reMsparse\n",
      "\n",
      "print \"Is the two methods return the same results?\", np.allclose(reMregular, reMsparse)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reconstructed matrix M' by scipy.linagl.svd is: [[ 0.162  0.400  0.379  0.468  0.176 -0.053 -0.115 -0.159 -0.092]\n",
        " [ 0.141  0.370  0.329  0.400  0.165 -0.033 -0.071 -0.097 -0.043]\n",
        " [ 0.152  0.505  0.358  0.410  0.236  0.024  0.060  0.087  0.124]\n",
        " [ 0.258  0.841  0.606  0.697  0.392  0.033  0.083  0.122  0.187]\n",
        " [ 0.449  1.234  1.051  1.266  0.556 -0.074 -0.155 -0.210 -0.049]\n",
        " [ 0.160  0.582  0.375  0.417  0.277  0.056  0.132  0.189  0.217]\n",
        " [ 0.160  0.582  0.375  0.417  0.277  0.056  0.132  0.189  0.217]\n",
        " [ 0.218  0.550  0.511  0.628  0.243 -0.065 -0.143 -0.197 -0.108]\n",
        " [ 0.097  0.532  0.230  0.212  0.267  0.137  0.315  0.444  0.425]\n",
        " [-0.061  0.232 -0.139 -0.266  0.145  0.240  0.546  0.767  0.664]\n",
        " [-0.065  0.335 -0.146 -0.301  0.203  0.306  0.695  0.977  0.849]\n",
        " [-0.043  0.254 -0.097 -0.208  0.152  0.221  0.503  0.707  0.616]]\n",
        "Reconstructed matrix M' by sparsesvd is: [[ 0.162  0.400  0.379  0.468  0.176 -0.053 -0.115 -0.159 -0.092]\n",
        " [ 0.141  0.370  0.329  0.400  0.165 -0.033 -0.071 -0.097 -0.043]\n",
        " [ 0.152  0.505  0.358  0.410  0.236  0.024  0.060  0.087  0.124]\n",
        " [ 0.258  0.841  0.606  0.697  0.392  0.033  0.083  0.122  0.187]\n",
        " [ 0.449  1.234  1.051  1.266  0.556 -0.074 -0.155 -0.210 -0.049]\n",
        " [ 0.160  0.582  0.375  0.417  0.277  0.056  0.132  0.189  0.217]\n",
        " [ 0.160  0.582  0.375  0.417  0.277  0.056  0.132  0.189  0.217]\n",
        " [ 0.218  0.550  0.511  0.628  0.243 -0.065 -0.143 -0.197 -0.108]\n",
        " [ 0.097  0.532  0.230  0.212  0.267  0.137  0.315  0.444  0.425]\n",
        " [-0.061  0.232 -0.139 -0.266  0.145  0.240  0.546  0.767  0.664]\n",
        " [-0.065  0.335 -0.146 -0.301  0.203  0.306  0.695  0.977  0.849]\n",
        " [-0.043  0.254 -0.097 -0.208  0.152  0.221  0.503  0.707  0.616]]\n",
        "Is the two methods return the same results? True\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.stats as st\n",
      "rhoorig, pvalorig = st.spearmanr(M)\n",
      "print \"Pairwise correlation matrix for the original matrix M is\", rhoorig\n",
      "\n",
      "rhorecon, pvalrecon = st.spearmanr(reMregular)\n",
      "print \"Pairwise correlation matrix for the reconstructed matrix M' is\", rhorecon"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pairwise correlation matrix for the original matrix M is [[ 1.000 -0.192  0.000  0.073 -0.333 -0.174 -0.258 -0.333 -0.333]\n",
        " [-0.192  1.000  0.000 -0.127  0.577 -0.302 -0.447 -0.577 -0.192]\n",
        " [ 0.000  0.000  1.000  0.438  0.000 -0.213 -0.316 -0.408 -0.408]\n",
        " [ 0.073 -0.127  0.438  1.000 -0.330 -0.172 -0.256 -0.330 -0.330]\n",
        " [-0.333  0.577  0.000 -0.330  1.000 -0.174 -0.258 -0.333 -0.333]\n",
        " [-0.174 -0.302 -0.213 -0.172 -0.174  1.000  0.674  0.522 -0.174]\n",
        " [-0.258 -0.447 -0.316 -0.256 -0.258  0.674  1.000  0.775  0.258]\n",
        " [-0.333 -0.577 -0.408 -0.330 -0.333  0.522  0.775  1.000  0.556]\n",
        " [-0.333 -0.192 -0.408 -0.330 -0.333 -0.174  0.258  0.556  1.000]]\n",
        "Pairwise correlation matrix for the reconstructed matrix M' is [[ 1.000  0.846  1.000  0.998  0.719 -0.837 -0.837 -0.837 -0.802]\n",
        " [ 0.846  1.000  0.846  0.844  0.972 -0.557 -0.557 -0.557 -0.480]\n",
        " [ 1.000  0.846  1.000  0.998  0.719 -0.837 -0.837 -0.837 -0.802]\n",
        " [ 0.998  0.844  0.998  1.000  0.718 -0.839 -0.839 -0.839 -0.804]\n",
        " [ 0.719  0.972  0.719  0.718  1.000 -0.389 -0.389 -0.389 -0.298]\n",
        " [-0.837 -0.557 -0.837 -0.839 -0.389  1.000  1.000  1.000  0.979]\n",
        " [-0.837 -0.557 -0.837 -0.839 -0.389  1.000  1.000  1.000  0.979]\n",
        " [-0.837 -0.557 -0.837 -0.839 -0.389  1.000  1.000  1.000  0.979]\n",
        " [-0.802 -0.480 -0.802 -0.804 -0.298  0.979  0.979  0.979  1.000]]\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "oriG1 = M[:,:5] \n",
      "oriG2 = M[:,5:]\n",
      "recoG1 = reMregular[:,:5] \n",
      "recoG2 = reMregular[:,5:] \n",
      "# average within group correlation for G1, G2\n",
      "r1, p1 = st.spearmanr(oriG1)\n",
      "# Reshape the correlation matrix into a lower triangle matrix without diagnol values\n",
      "# such that the self-correlation is not included and correlation is not duplicated\n",
      "lowertri_r1 = np.tril(r1,-1)\n",
      "# Calculate the within group correlation for G1 (original M)\n",
      "# ((r1.shape[0] ** 2 - r1.shape[0])/2) is the total number of correlation values\n",
      "withincorG1or = lowertri_r1.sum()/((r1.shape[0] ** 2 - r1.shape[0])/2)\n",
      "\n",
      "r2, p2 = st.spearmanr(oriG2)\n",
      "lowertri_r2 = np.tril(r2,-1)\n",
      "withincorG2or = lowertri_r2.sum()/((r2.shape[0] ** 2 - r2.shape[0])/2)\n",
      "\n",
      "r3, p3 = st.spearmanr(recoG1)\n",
      "lowertri_r3 = np.tril(r3,-1)\n",
      "withincorG1re = lowertri_r3.sum()/((r3.shape[0] ** 2 - r3.shape[0])/2)\n",
      "\n",
      "r4, p4 = st.spearmanr(recoG2)\n",
      "lowertri_r4 = np.tril(r4,-1)\n",
      "withincorG2re = lowertri_r4.sum()/((r4.shape[0] ** 2 - r4.shape[0])/2)\n",
      "\n",
      "print \"Within group correlation for G1 (original M) is\", withincorG1or\n",
      "print \"Within group correlation for G2 (original M) is\", withincorG2or\n",
      "\n",
      "print \"Within group correlation for G1 (reduced M) is\", withincorG1re\n",
      "print \"Within group correlation for G2 (reduced M) is\", withincorG2re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Within group correlation for G1 (original M) is 0.0105776866299\n",
        "Within group correlation for G2 (original M) is 0.43511771482\n",
        "Within group correlation for G1 (reduced M) is 0.866042884512\n",
        "Within group correlation for G2 (reduced M) is 0.98951048951\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the average cross-group correlation for G1-G2 \n",
      "# Just need to select one part of the pairwise correlation matrix (the first 5 rows with the last 4 columns)\n",
      "# And calculate the average\n",
      "print \"Average cross-group correlation for G1-G2 (original M) is\", rhoorig[:5, 5:].sum()/(rhoorig[:5, 5:].shape[0]*rhoorig[:5, 5:].shape[1])\n",
      "print \"Average cross-group correlation for G1-G2 (reduced M) is\", rhorecon[:5, 5:].sum()/(rhorecon[:5, 5:].shape[0]*rhorecon[:5, 5:].shape[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Average cross-group correlation for G1-G2 (original M) is -0.307562188906\n",
        "Average cross-group correlation for G1-G2 (reduced M) is -0.678168764439\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 4 (20 points)**. Clustering with LSA\n",
      "\n",
      "1. Begin by loading a pubmed database of selected article titles using 'cPickle'. With the following:\n",
      "```import cPickle\n",
      "docs = cPickle.load(open('pubmed.pic'))```\n",
      "\n",
      "    Create a tf-idf matrix for every term that appears at least once in any of the documents. What is the shape of the tf-idf matrix? \n",
      "\n",
      "2. Perform SVD on the tf-idf matrix to obtain $U \\Sigma V^T$ (often written as $T \\Sigma D^T$ in this context with $T$ representing the terms and $D$ representing the documents). If we set all but the top $k$ singular values to 0, the reconstructed matrix is essentially $U_k \\Sigma_k V_k^T$, where $U_k$ is $m \\times k$, $\\Sigma_k$ is $k \\times k$ and $V_k^T$ is $k \\times n$. Terms in this reduced space are represented by $U_k \\Sigma_k$ and documents by $\\Sigma_k V^T_k$. Reconstruct the matrix using the first $k=10$ singular values.\n",
      "\n",
      "3. Use agglomerative hierachical clustering with complete linkage to plot a dendrogram and comment on the likely number of  document clusters with $k = 100$. Use the dendrogram function from [SciPy ](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.cluster.hierarchy.dendrogram.html).\n",
      "\n",
      "4. Determine how similar each of the original documents is to the new document `mystery.txt`. Since $A = U \\Sigma V^T$, we also have $V = A^T U S^{-1}$ using orthogonality and the rule for transposing matrix products. This suggests that in order to map the new document to the same concept space, first find the tf-idf vector $v$ for the new document - this must contain all (and only) the terms present in the existing tf-idx matrix. Then the query vector $q$ is given by $v^T U_k \\Sigma_k^{-1}$. Find the 10 documents most similar to the new document and the 10 most dissimilar. \n",
      "\n",
      "5. Many documents often have some boilerplate material such as organization information, Copyright, etc. at the front or back of the document. Does it matter that the front and back matter of each document is essentially identical for either LSA-based clustering (part 3) or information retrieval (part 4)? Why or why not?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here\n",
      "# 1) \n",
      "import cPickle\n",
      "docs = cPickle.load(open('pubmed.pic'))\n",
      "\n",
      "tf_idf_matrix = tf_idf(docs)\n",
      "\n",
      "print \"The shape of the tf-idf matrix is\", tf_idf_matrix.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The shape of the tf-idf matrix is (6489, 178)\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 2)\n",
      "reduced = re_sparse(tf_idf_matrix, 10)\n",
      "print reduced\n",
      "print reduced.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.399  0.670  0.985 ...,  0.279  0.683  0.573]\n",
        " [ 0.090  0.142  0.067 ..., -0.260  0.049  0.058]\n",
        " [ 0.007  0.027  0.013 ...,  0.084  0.014  0.005]\n",
        " ..., \n",
        " [ 0.009  0.019  0.016 ...,  0.007  0.005  0.009]\n",
        " [ 0.043  0.109  0.081 ...,  0.166  0.042  0.039]\n",
        " [ 0.010  0.025  0.019 ...,  0.017  0.008  0.010]]\n",
        "(6489, 178)\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 3) \n",
      "from scipy.cluster.hierarchy import dendrogram\n",
      "from scipy.cluster.hierarchy import linkage\n",
      "from scipy.spatial.distance import pdist,squareform\n",
      "\n",
      "# Using K =100 to reduce tf_idf matrix\n",
      "reduced100 = re_sparse(tf_idf_matrix, 100)\n",
      "print reduced100.shape\n",
      "\n",
      "# First derive the pair-wise distance matrix for reduced tf_idf matrix\n",
      "dist = pd.DataFrame(squareform(pdist(reduced100.T, metric='euclidean')))\n",
      "\n",
      "# Then get the complete linkage matrix using distance matrix\n",
      "clusters = linkage(dist, method='complete')\n",
      "clustersdata = pd.DataFrame(clusters, columns=['1', '2', 'distance between 1&2', '#. of items in clusterer'], index=['cluster %d' %(i+1) for i in range(clusters.shape[0])])\n",
      "print \"The number of document clusters with k=100 is\", clustersdata.shape[0]\n",
      "print clustersdata\n",
      "dendplot = dendrogram(clusters)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(6489, 178)\n",
        "The number of document clusters with k=100 is"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 177\n",
        "               1    2  distance between 1&2  #. of items in clusterer\n",
        "cluster 1     33  124              9.149393                         2\n",
        "cluster 2     56  178             11.652426                         3\n",
        "cluster 3     11  163             13.008313                         2\n",
        "cluster 4    122  176             14.464237                         2\n",
        "cluster 5    136  180             14.640553                         3\n",
        "cluster 6      8   10             15.485192                         2\n",
        "cluster 7    181  183             17.230629                         4\n",
        "cluster 8     67   79             17.808536                         2\n",
        "cluster 9      7  179             17.847875                         4\n",
        "cluster 10     4  104             19.261548                         2\n",
        "cluster 11    35  173             19.568476                         2\n",
        "cluster 12    64  182             21.659020                         4\n",
        "cluster 13    27  187             22.359677                         3\n",
        "cluster 14    92  188             24.239125                         3\n",
        "cluster 15   170  185             24.443315                         3\n",
        "cluster 16    60  114             24.504069                         2\n",
        "cluster 17   189  190             24.881413                         7\n",
        "cluster 18    13  141             25.281542                         2\n",
        "cluster 19   140  142             25.289024                         2\n",
        "cluster 20   192  194             27.690593                        10\n",
        "cluster 21    91  111             28.136967                         2\n",
        "cluster 22   184  186             28.279169                         8\n",
        "cluster 23    12  174             29.057706                         2\n",
        "cluster 24    19  193             31.313004                         3\n",
        "cluster 25    25   86             31.726536                         2\n",
        "cluster 26   191  197             31.864160                        13\n",
        "cluster 27    66  195             32.409529                         3\n",
        "cluster 28   138  196             32.897835                         3\n",
        "cluster 29   200  201             33.343734                         5\n",
        "cluster 30   199  203             35.442371                        21\n",
        "...          ...  ...                   ...                       ...\n",
        "cluster 148  267  297            178.736138                        20\n",
        "cluster 149   69  317            179.510911                         3\n",
        "cluster 150  307  315            181.485642                        20\n",
        "cluster 151   48  144            183.963454                         2\n",
        "cluster 152  150  175            184.106271                         2\n",
        "cluster 153  324  326            185.975454                        13\n",
        "cluster 154   62  131            188.114304                         2\n",
        "cluster 155  328  331            189.074934                         4\n",
        "cluster 156  304  327            189.260882                        37\n",
        "cluster 157   72  332            191.758080                         5\n",
        "cluster 158  153  154            195.070708                         2\n",
        "cluster 159  329  335            196.895767                         4\n",
        "cluster 160  276  314            199.314590                        17\n",
        "cluster 161  143  336            203.610028                         5\n",
        "cluster 162   41  338            210.919093                         6\n",
        "cluster 163  330  334            228.753432                        18\n",
        "cluster 164   21  130            233.119267                         2\n",
        "cluster 165   47   96            270.264194                         2\n",
        "cluster 166  325  333            270.585093                        57\n",
        "cluster 167  269  292            272.217869                        72\n",
        "cluster 168  339  341            288.278212                         8\n",
        "cluster 169   52  105            300.264258                         2\n",
        "cluster 170  108  346            327.746802                         3\n",
        "cluster 171  342  347            393.698045                         5\n",
        "cluster 172  340  343            414.833645                        75\n",
        "cluster 173  337  344            423.852792                        89\n",
        "cluster 174  345  349            640.856445                        83\n",
        "cluster 175   54  348            912.339167                         6\n",
        "cluster 176  350  351           1047.862135                       172\n",
        "cluster 177  352  353           2136.903388                       178\n",
        "\n",
        "[177 rows x 4 columns]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEBCAYAAACXArmGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBGRYFAgIiLx/wEoapWkrbkSUAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f55fe40af90>"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 4) \n",
      "# Open new document\n",
      "m = open(\"mystery.txt\").read().replace('-',' ').translate(None, string.punctuation).lower().split()\n",
      "# Find the index in original tf_idf_matrix\n",
      "ind = list(tf_idf_matrix.index)\n",
      "# make sure that the tf of new document contains all (and only) the \n",
      "# terms present in the existing matrix.\n",
      "newtfini = [x for x in m if x in ind]\n",
      "\n",
      "newtf = pd.DataFrame(index=ind,columns=['mystery']).fillna(0)\n",
      "for i in newtfini:\n",
      "    newtf.loc[i] = newtf.loc[i] + 1\n",
      "\n",
      "# find the tf-idf vector for the new document\n",
      "# Slightly change the tf-idf function\n",
      "def tf_idf(docs):\n",
      "    termfreq = pd.DataFrame(newtf)\n",
      "    invdocfreq = idf(docs)\n",
      "    for i in range(len(termfreq)):\n",
      "        termfreq.ix[i] = termfreq.ix[i] * invdocfreq.ix[i,0]\n",
      "    return termfreq\n",
      "\n",
      "# Now we can get the tf-idf vector for the new document\n",
      "tf_idf_new_vec = tf_idf(docs)\n",
      "print newtf.shape\n",
      "print tf_idf_new_vec.shape\n",
      "print tf_idf_new_vec\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(6489, 1)\n",
        "(6489, 1)\n",
        "                      mystery\n",
        "                     0.000000\n",
        "0                    0.000000\n",
        "0000028              0.000000\n",
        "00001                0.000000\n",
        "0001                 0.000000\n",
        "00011                0.000000\n",
        "00013                0.000000\n",
        "0002                 0.000000\n",
        "00022                0.000000\n",
        "0003                 0.000000\n",
        "0008                 0.000000\n",
        "0009                 0.000000\n",
        "001                  0.000000\n",
        "0017                 0.000000\n",
        "002                  0.000000\n",
        "0023                 0.000000\n",
        "0026                 0.000000\n",
        "003                  0.000000\n",
        "00352                0.000000\n",
        "004                  0.000000\n",
        "0047                 0.000000\n",
        "0048                 0.000000\n",
        "005                  0.000000\n",
        "006                  0.000000\n",
        "01                   0.000000\n",
        "013                  0.000000\n",
        "014                  0.000000\n",
        "015                  0.000000\n",
        "016                  0.000000\n",
        "017                  0.000000\n",
        "...                       ...\n",
        "would                0.000000\n",
        "wound                0.000000\n",
        "wst                  0.000000\n",
        "wt                   0.000000\n",
        "wwwisrctncom         0.000000\n",
        "x                    0.000000\n",
        "x109l                0.000000\n",
        "xcd4cre              0.000000\n",
        "xenogenic            0.000000\n",
        "xenotransplantation  0.000000\n",
        "xxx                  0.000000\n",
        "xylulose             0.000000\n",
        "y                    0.000000\n",
        "yahr                 0.000000\n",
        "yd                   0.000000\n",
        "year                 2.783888\n",
        "yearbooks            0.000000\n",
        "years                5.548737\n",
        "yet                  0.000000\n",
        "yield                0.000000\n",
        "yielded              0.000000\n",
        "yl                   0.000000\n",
        "ymca                 0.000000\n",
        "yoelii               0.000000\n",
        "young                0.000000\n",
        "younger              0.000000\n",
        "youth                0.000000\n",
        "youths               0.000000\n",
        "zno                  0.000000\n",
        "zoo                  0.000000\n",
        "\n",
        "[6489 rows x 1 columns]\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sparse SVD\n",
      "# Note that I'm confused about where to use K=10 OR K=100 for this question\n",
      "# So I just use K=10 AND the results would be different for K=100\n",
      "# But the logic is just like this\n",
      "U, s, V = sparsesvd(csc_matrix(tf_idf_matrix),100)\n",
      "q = tf_idf_new_vec.T.dot(U.T).dot(np.linalg.inv(np.diag(s))).T\n",
      "C = tf_idf_matrix.T.dot(U.T).dot(np.linalg.inv(np.diag(s))).T\n",
      "C['mystery'] = q\n",
      "C = np.array(C)\n",
      "# Find the 10 documents most similar to the new document and the 10 most dissimilar\n",
      "cosine_similarity = 1 - distance_looping_column(C, cosine)\n",
      "cosine_similarity = cosine_similarity[cosine_similarity.shape[0] - 1, :cosine_similarity.shape[0] - 1]\n",
      "cosine_similarity = pd.DataFrame(cosine_similarity, index = list(tf_idf_matrix.columns), columns = ['cosine_similarity']).sort(['cosine_similarity'], ascending = False)\n",
      "sim = list(cosine_similarity.index)\n",
      "\n",
      "most_sim = sim[:10]\n",
      "most_dissim = sim[-10:]\n",
      "print \"The 10 documents most similar to the new document are\", most_sim\n",
      "print \"The 10 documents most dissimilar to the new document are\", most_dissim"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The 10 documents most similar to the new document are ['Diabetes Numeracy and Blood Glucose Control: Association With Type of Diabetes and Source of Care.', 'Feasibility of the SMART Project: A Text Message Program for Adolescents With Type 1 Diabetes.', 'Health Care Utilization Among U.S. Adults With Diagnosed Diabetes, 2013.', 'Outcomes of a Diabetes Education Program for Registered Nurses Caring for Individuals With Diabetes.', 'Effect of Self-Efficacy on Weight Loss: A Psychosocial Analysis of a Community-Based Adaptation of the Diabetes Prevention Program Lifestyle Intervention.', '\"Living Well with Diabetes\": Evaluation of a Pilot Program to Promote Diabetes Prevention and Self-Management in a Medically Underserved Community.', 'Efficacy and Safety of Saxagliptin as Add-On Therapy in Type 2 Diabetes.', 'Prevalence and Determinants of Anemia in Older People With Diabetes Attending an Outpatient Clinic: A Cross-Sectional Audit.', 'Disparities in Postpartum Follow-Up in Women With Gestational Diabetes Mellitus.', 'Demographic Disparities Among Medicare Beneficiaries with Type 2 Diabetes Mellitus in 2011: Diabetes Prevalence, Comorbidities, and Hypoglycemia Events.']\n",
        "The 10 documents most dissimilar to the new document are ['Avian haemosporidians from Neotropical highlands: Evidence from morphological and molecular data.', 'Improving Lateral-Flow Immunoassay (LFIA) Diagnostics via Biomarker Enrichment for mHealth.', 'Cerebral malaria as a risk factor for the development of epilepsy and other long-term neurological conditions: a meta-analysis.', 'CD4 T-cell subsets in malaria: TH1/TH2 revisited.', 'Nerve Growth Factor Potentiates Nicotinic Synaptic Transmission in Mouse Airway Parasympathetic Neurons.', 'In situ hybridization and sequence analysis reveal an association of Plasmodium spp. with mortalities in wild passerine birds in Austria.', 'Crystal Structures of the Carboxyl cGMP Binding Domain of the Plasmodium falciparum cGMP-dependent Protein Kinase Reveal a Novel Capping Triad Crucial for Merozoite Egress.', 'Antibodies to the Plasmodium falciparum proteins MSPDBL1 and MSPDBL2 opsonise merozoites, inhibit parasite growth and predict protection from clinical malaria.', 'IRGM3 contributes to immunopathology and is required for differentiation of antigen-specific effector CD8+ T cells in experimental cerebral malaria.', 'CD40 Is Required for Protective Immunity against Liver Stage Plasmodium Infection.']\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 5) \n",
      "print \"So the boilerplate materials do have something to do with LSA-based clustering and information retrieval, and the documents with same boilerplate materials tend to be clustered together, while new document will show bias towards those documents that have same boilerplate materials, but I think this effect is small at least for the documents we are handling here, cuz the boilerplate materials have smaller length than the remaining contents.\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notes on the Pubmed articles\n",
      "----\n",
      "\n",
      "These were downloaded with the following script.\n",
      "\n",
      "```python\n",
      "from Bio import Entrez, Medline\n",
      "Entrez.email = \"YOUR EMAIL HERE\"\n",
      "import cPickle\n",
      "\n",
      "try:\n",
      "    docs = cPickle.load(open('pubmed.pic'))\n",
      "except Exception, e:\n",
      "    print e\n",
      "\n",
      "    docs = {}\n",
      "    for term in ['plasmodium', 'diabetes', 'asthma', 'cytometry']:\n",
      "        handle = Entrez.esearch(db=\"pubmed\", term=term, retmax=50)\n",
      "        result = Entrez.read(handle)\n",
      "        handle.close()\n",
      "        idlist = result[\"IdList\"]\n",
      "        handle2 = Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\", retmode=\"text\")\n",
      "        result2 = Medline.parse(handle2)\n",
      "        for record in result2:\n",
      "            title = record.get(\"TI\", None)\n",
      "            abstract = record.get(\"AB\", None)\n",
      "            if title is None or abstract is None:\n",
      "                continue\n",
      "            docs[title] = '\\n'.join([title, abstract])\n",
      "            print title\n",
      "        handle2.close()\n",
      "    cPickle.dump(docs, open('pubmed.pic', 'w'))\n",
      "docs.values()\n",
      "```"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}